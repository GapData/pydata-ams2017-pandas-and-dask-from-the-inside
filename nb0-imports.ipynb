{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dask talk outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas runs out of memory on big data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ingest/import/load\n",
    "2. Filter\n",
    "3. Index\n",
    "4. Subtotal\n",
    "5. Aggregate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So split computations into pieces\n",
    "\n",
    "Works particularly well with historic timeseries data, where each day/week/month is a new dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a faster storage format, especially one that is chunked and supports filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv is slow and take a lot of disk space\n",
    "compressed csv is slow, smaller but not seekable\n",
    "hdf5, bcolz, and more recently parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastparquet.from_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run calculations across multiple cores/computers/in a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:\n",
    "    - Hadoop\n",
    "    - PySpark\n",
    "    - xxxx\n",
    "    - MapReduce\n",
    "    - JPM's lib.dist.simpleApply(fn, dataSrc, bucketSize, fargs, fkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about if we want to keep using pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a drop-in replacement for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv('flights-2016-*.xz')\n",
    "res = ddf[ddf.Carrier='AA'].groupby('FlightDate').sum()[['Flights','Cancelled']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dask to lazily load csv data too big for memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising progress of Dask calculations\n",
    "\n",
    "\n",
    "ProgressBar\n",
    "\n",
    "Profiler,  # Task profiler\n",
    "ResourceProfiler, \n",
    "CacheProfiler(metric=cachey.nbytes)\n",
    "```\n",
    "#All store results in a list of named tuples\n",
    "import dask.diagnostics\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.diagnostics import Profiler as TP # Task profiler\n",
    "from dask.diagnostics import ResourceProfiler as RP\n",
    "from dask.diagnostics import CacheProfiler as CP\n",
    "from cachey import nbytes\n",
    "\n",
    "with ProgressBar():\n",
    "    with TP() as tp, RP(dt=0.25) as rp, CP(metric=nbytes) as cp:\n",
    "        res = task.compute()\n",
    "\n",
    "import bokeh\n",
    "bokeh.io.output notebook()\n",
    "dask.diagnostics.visualize([tp, rp, cp], save=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding recalculations by caching\n",
    "\n",
    "http://dask.readthedocs.io/en/latest/caching.html\n",
    "https://github.com/dask/cachey\n",
    "    \n",
    "Dask forgets intermediate results\n",
    "```\n",
    "t1 = ddf.groupby['FlightDate'].sum().Flights\n",
    "t2 = ddf.groupby['FlightDate'].sum().Cancelled\n",
    "\n",
    "t1.compute()\n",
    "t2.compute()\n",
    "\n",
    "dd.compute(t1, t2)\n",
    "\n",
    "from dask.cache import Cache\n",
    "cache = Cache(1e9)\n",
    "cash.register()\n",
    "\n",
    "```\n",
    "Caching strategies:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed scheduling\n",
    "\n",
    "From https://github.com/dask/dask/issues/1040\n",
    "\n",
    "```\n",
    "pip install distributed\n",
    "dscheduler\n",
    "dworker 127.0.0.1:8786 --nprocs 8\n",
    "\n",
    ">>> from distributed import Executor\n",
    ">>> e = Executor('127.0.0.1:8786')\n",
    ">>> ...\n",
    ">>> count, total = dask.compute(G.count(), G.sum(), get=e.get)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
